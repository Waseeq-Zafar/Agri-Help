{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "77a61443",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "neuralforecast 3.0.2 requires torch<=2.6.0,>=2.0.0, but you have torch 2.8.0 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install -q --upgrade torch torchvision torchaudio\n",
    "! pip install -q scikit-learn yfinance pandas matplotlib numpy tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5e6ec730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.8.0+cpu\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check PyTorch version\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9db59bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  3 of 3 completed\n",
      "[*********************100%***********************]  3 of 3 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data...\n",
      "Data shape: (1257, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Corn</th>\n",
       "      <th>Soybeans</th>\n",
       "      <th>Wheat</th>\n",
       "      <th>DE</th>\n",
       "      <th>ADM</th>\n",
       "      <th>CTVA</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-01-02</th>\n",
       "      <td>485.25</td>\n",
       "      <td>391.50</td>\n",
       "      <td>944.25</td>\n",
       "      <td>39.126690</td>\n",
       "      <td>26.820271</td>\n",
       "      <td>164.021683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-03</th>\n",
       "      <td>475.00</td>\n",
       "      <td>386.50</td>\n",
       "      <td>930.50</td>\n",
       "      <td>39.050316</td>\n",
       "      <td>26.502977</td>\n",
       "      <td>162.806778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-06</th>\n",
       "      <td>477.25</td>\n",
       "      <td>384.75</td>\n",
       "      <td>932.75</td>\n",
       "      <td>38.744850</td>\n",
       "      <td>26.465651</td>\n",
       "      <td>163.975266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-07</th>\n",
       "      <td>475.00</td>\n",
       "      <td>384.50</td>\n",
       "      <td>935.00</td>\n",
       "      <td>38.278133</td>\n",
       "      <td>26.568302</td>\n",
       "      <td>161.128113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-08</th>\n",
       "      <td>480.00</td>\n",
       "      <td>384.25</td>\n",
       "      <td>938.25</td>\n",
       "      <td>37.853870</td>\n",
       "      <td>26.782942</td>\n",
       "      <td>163.131378</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Corn  Soybeans   Wheat         DE        ADM        CTVA\n",
       "Date                                                                  \n",
       "2020-01-02  485.25    391.50  944.25  39.126690  26.820271  164.021683\n",
       "2020-01-03  475.00    386.50  930.50  39.050316  26.502977  162.806778\n",
       "2020-01-06  477.25    384.75  932.75  38.744850  26.465651  163.975266\n",
       "2020-01-07  475.00    384.50  935.00  38.278133  26.568302  161.128113\n",
       "2020-01-08  480.00    384.25  938.25  37.853870  26.782942  163.131378"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "commodity_tickers = [\"ZC=F\", \"ZS=F\", \"KE=F\"]\n",
    "stock_tickers = [\"DE\", \"ADM\", \"CTVA\"]\n",
    "\n",
    "print(\"Downloading data...\")\n",
    "data_commodities = yf.download(commodity_tickers, start=\"2020-01-01\", end=\"2024-12-31\")['Close']\n",
    "data_stocks = yf.download(stock_tickers, start=\"2020-01-01\", end=\"2024-12-31\")['Close']\n",
    "\n",
    "data = pd.concat([data_commodities, data_stocks], axis=1)\n",
    "data.columns = ['Corn', 'Soybeans', 'Wheat', 'DE', 'ADM', 'CTVA']\n",
    "data = data.fillna(method='ffill').dropna()\n",
    "\n",
    "print(f\"Data shape: {data.shape}\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "85fc619a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "data_scaled = scaler.fit_transform(data.values)\n",
    "\n",
    "prediction_length = 30\n",
    "context_length = 90\n",
    "num_features = data.shape[1]\n",
    "\n",
    "# ----- Sequence Creation -----\n",
    "def create_sequences(data, context_length, prediction_length):\n",
    "    X, y = [], []\n",
    "    L = len(data)\n",
    "    for i in range(L - context_length - prediction_length + 1):\n",
    "        X.append(data[i:i+context_length])\n",
    "        y.append(data[i+context_length:i+context_length+prediction_length])\n",
    "    return np.array(X, dtype=np.float32), np.array(y, dtype=np.float32)\n",
    "\n",
    "X, y = create_sequences(data_scaled, context_length, prediction_length)\n",
    "\n",
    "# ----- Train / Val / Test Split -----\n",
    "train_size = int(len(X) * 0.7)\n",
    "val_size = int(len(X) * 0.15)\n",
    "\n",
    "X_train, y_train = X[:train_size], y[:train_size]\n",
    "X_val, y_val = X[train_size:train_size+val_size], y[train_size:train_size+val_size]\n",
    "X_test, y_test = X[train_size+val_size:], y[train_size+val_size:]\n",
    "\n",
    "# ----- TensorFlow Datasets -----\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(1000).batch(batch_size)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(batch_size)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b021c424",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential([\n",
    "    layers.Input(shape=(context_length, num_features)),\n",
    "    layers.LSTM(128, return_sequences=True),\n",
    "    layers.GRU(128),\n",
    "    layers.Dense(prediction_length * num_features),\n",
    "    layers.Reshape((prediction_length, num_features))\n",
    "])\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(1e-3), loss=\"mse\")\n",
    "\n",
    "# ----- Training -----\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=15,\n",
    "    restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2fa5713d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 76ms/step - loss: 0.1069 - val_loss: 0.0309\n",
      "Epoch 2/60\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 59ms/step - loss: 0.0134 - val_loss: 0.0242\n",
      "Epoch 3/60\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 60ms/step - loss: 0.0102 - val_loss: 0.0276\n",
      "Epoch 4/60\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 60ms/step - loss: 0.0096 - val_loss: 0.0264\n",
      "Epoch 5/60\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - loss: 0.0092 - val_loss: 0.0254\n",
      "Epoch 6/60\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - loss: 0.0085 - val_loss: 0.0214\n",
      "Epoch 7/60\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 63ms/step - loss: 0.0080 - val_loss: 0.0274\n",
      "Epoch 8/60\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 60ms/step - loss: 0.0074 - val_loss: 0.0221\n",
      "Epoch 9/60\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - loss: 0.0063 - val_loss: 0.0210\n",
      "Epoch 10/60\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - loss: 0.0060 - val_loss: 0.0173\n",
      "Epoch 11/60\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - loss: 0.0054 - val_loss: 0.0200\n",
      "Epoch 12/60\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - loss: 0.0054 - val_loss: 0.0196\n",
      "Epoch 13/60\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 64ms/step - loss: 0.0049 - val_loss: 0.0186\n",
      "Epoch 14/60\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 66ms/step - loss: 0.0047 - val_loss: 0.0150\n",
      "Epoch 15/60\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - loss: 0.0045 - val_loss: 0.0237\n",
      "Epoch 16/60\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - loss: 0.0044 - val_loss: 0.0158\n",
      "Epoch 17/60\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - loss: 0.0047 - val_loss: 0.0196\n",
      "Epoch 18/60\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - loss: 0.0040 - val_loss: 0.0188\n",
      "Epoch 19/60\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 66ms/step - loss: 0.0036 - val_loss: 0.0195\n",
      "Epoch 20/60\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 65ms/step - loss: 0.0036 - val_loss: 0.0197\n",
      "Epoch 21/60\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 141ms/step - loss: 0.0033 - val_loss: 0.0180\n",
      "Epoch 22/60\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 158ms/step - loss: 0.0032 - val_loss: 0.0214\n",
      "Epoch 23/60\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 179ms/step - loss: 0.0031 - val_loss: 0.0230\n",
      "Epoch 24/60\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 163ms/step - loss: 0.0028 - val_loss: 0.0222\n",
      "Epoch 25/60\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 144ms/step - loss: 0.0028 - val_loss: 0.0231\n",
      "Epoch 26/60\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 178ms/step - loss: 0.0029 - val_loss: 0.0258\n",
      "Epoch 27/60\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 130ms/step - loss: 0.0025 - val_loss: 0.0229\n",
      "Epoch 28/60\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 139ms/step - loss: 0.0025 - val_loss: 0.0246\n",
      "Epoch 29/60\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 146ms/step - loss: 0.0023 - val_loss: 0.0282\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.0217\n",
      "Test Loss: 0.021745163947343826\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 638ms/step\n",
      "Forecast shape: (30, 6)\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=60,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ----- Evaluation -----\n",
    "test_loss = model.evaluate(test_dataset)\n",
    "print(\"Test Loss:\", test_loss)\n",
    "\n",
    "# ----- Example Forecast -----\n",
    "y_pred = model.predict(X_test[:1])\n",
    "y_pred_rescaled = scaler.inverse_transform(y_pred[0])\n",
    "print(\"Forecast shape:\", y_pred_rescaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "005fa1e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 573.40295 ,  489.84708 , 1195.3168  ,   53.425842,   45.833378,\n",
       "         314.95584 ],\n",
       "       [ 607.0914  ,  484.49164 , 1201.9888  ,   55.382465,   43.940548,\n",
       "         318.07816 ],\n",
       "       [ 567.9296  ,  491.34586 , 1198.8961  ,   53.710236,   44.695015,\n",
       "         323.34552 ],\n",
       "       [ 604.1428  ,  494.69293 , 1225.5104  ,   54.508118,   44.498817,\n",
       "         322.5391  ],\n",
       "       [ 614.5661  ,  494.84076 , 1215.3511  ,   54.72854 ,   45.44645 ,\n",
       "         331.14584 ],\n",
       "       [ 596.7669  ,  504.66135 , 1212.0516  ,   55.109585,   43.73358 ,\n",
       "         322.36047 ],\n",
       "       [ 603.0707  ,  493.8207  , 1222.3516  ,   54.925613,   46.002483,\n",
       "         318.0702  ],\n",
       "       [ 599.5673  ,  502.6597  , 1233.5709  ,   53.01955 ,   45.103653,\n",
       "         324.30124 ],\n",
       "       [ 596.2206  ,  496.82346 , 1248.521   ,   53.272144,   44.97423 ,\n",
       "         320.68436 ],\n",
       "       [ 595.7841  ,  495.60916 , 1237.8087  ,   54.059628,   44.647762,\n",
       "         336.5586  ],\n",
       "       [ 600.20825 ,  486.2368  , 1239.7139  ,   55.24114 ,   44.586403,\n",
       "         318.18085 ],\n",
       "       [ 578.4584  ,  486.67014 , 1232.8468  ,   56.087242,   44.981815,\n",
       "         331.14368 ],\n",
       "       [ 602.7814  ,  496.85825 , 1231.4175  ,   56.684998,   44.430977,\n",
       "         328.96072 ],\n",
       "       [ 602.95703 ,  503.3871  , 1262.102   ,   55.023506,   44.031414,\n",
       "         322.09583 ],\n",
       "       [ 615.4749  ,  498.67035 , 1252.065   ,   55.020264,   45.074074,\n",
       "         320.78995 ],\n",
       "       [ 619.17206 ,  488.30496 , 1223.6033  ,   55.154507,   44.19995 ,\n",
       "         332.80057 ],\n",
       "       [ 615.9912  ,  500.3115  , 1243.0967  ,   54.11711 ,   45.276154,\n",
       "         321.87766 ],\n",
       "       [ 581.79956 ,  505.2776  , 1249.1084  ,   56.48859 ,   46.195213,\n",
       "         331.19815 ],\n",
       "       [ 590.89374 ,  515.98596 , 1242.6758  ,   53.699028,   45.68304 ,\n",
       "         329.566   ],\n",
       "       [ 610.20404 ,  510.94888 , 1255.2383  ,   56.578365,   44.36911 ,\n",
       "         329.06448 ],\n",
       "       [ 617.72864 ,  504.9392  , 1240.2368  ,   54.517525,   44.565434,\n",
       "         333.42325 ],\n",
       "       [ 615.55566 ,  507.6886  , 1253.7148  ,   55.687534,   44.96989 ,\n",
       "         338.1344  ],\n",
       "       [ 621.257   ,  494.16858 , 1254.8425  ,   56.760117,   45.271343,\n",
       "         333.10226 ],\n",
       "       [ 612.2329  ,  508.82205 , 1277.7993  ,   55.764267,   45.030174,\n",
       "         346.29572 ],\n",
       "       [ 627.3123  ,  506.13992 , 1257.7902  ,   57.262547,   45.838566,\n",
       "         337.26614 ],\n",
       "       [ 612.0254  ,  510.22635 , 1269.601   ,   55.174244,   45.01281 ,\n",
       "         336.15762 ],\n",
       "       [ 611.21533 ,  508.10098 , 1269.0936  ,   57.28705 ,   45.249435,\n",
       "         333.4403  ],\n",
       "       [ 605.77985 ,  502.4548  , 1258.8672  ,   57.00167 ,   44.556164,\n",
       "         341.97287 ],\n",
       "       [ 607.17975 ,  507.8908  , 1269.0604  ,   56.06673 ,   46.069633,\n",
       "         344.30725 ],\n",
       "       [ 664.10333 ,  514.8773  , 1260.189   ,   57.680138,   45.506863,\n",
       "         341.48932 ]], dtype=float32)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_rescaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "207888b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at: ../models/Commodities_price_Forcasting/hybrid_lstm_gru_tf.keras\n",
      "Scaler saved at: ../models/Commodities_price_Forcasting/scaler.pkl\n",
      "Forecasted future shape: (30, 6)\n",
      "[[ 468.04602   468.18494  1128.8984     47.939697   44.6193    321.7221  ]\n",
      " [ 504.1194    459.30927  1160.5737     50.10257    41.84441   335.4461  ]\n",
      " [ 431.7737    467.49683  1157.4352     49.701668   43.45735   338.61383 ]\n",
      " [ 511.75314   478.45938  1205.1013     50.13604    41.813538  333.82837 ]\n",
      " [ 542.16956   461.09055  1188.1763     50.901012   44.02994   348.09775 ]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import joblib \n",
    "\n",
    "save_dir = \"../models/Commodities_price_Forcasting/\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "model_save_path = os.path.join(save_dir, \"hybrid_lstm_gru_tf.keras\")\n",
    "scaler_save_path = os.path.join(save_dir, \"scaler.pkl\")\n",
    "\n",
    "model.save(model_save_path)\n",
    "print(f\"Model saved at: {model_save_path}\")\n",
    "\n",
    "joblib.dump(scaler, scaler_save_path)\n",
    "print(f\"Scaler saved at: {scaler_save_path}\")\n",
    "\n",
    "\n",
    "def load_forecasting_model(path):\n",
    "    return tf.keras.models.load_model(path)\n",
    "\n",
    "def load_scaler(path):\n",
    "    return joblib.load(path)\n",
    "\n",
    "\n",
    "def forecast_future(model, recent_data, scaler, context_length, prediction_length, num_features):\n",
    "    input_seq = recent_data[-context_length:].reshape(1, context_length, num_features)\n",
    "    pred = model.predict(input_seq, verbose=0)\n",
    "    pred_rescaled = scaler.inverse_transform(pred[0])\n",
    "    return pred_rescaled\n",
    "\n",
    "\n",
    "\n",
    "loaded_model = load_forecasting_model(model_save_path)\n",
    "loaded_scaler = load_scaler(scaler_save_path)\n",
    "\n",
    "recent_data = data_scaled[-context_length:]\n",
    "\n",
    "future_steps = forecast_future(\n",
    "    loaded_model,\n",
    "    recent_data,\n",
    "    loaded_scaler,\n",
    "    context_length,\n",
    "    prediction_length,\n",
    "    num_features\n",
    ")\n",
    "\n",
    "print(\"Forecasted future shape:\", future_steps.shape)\n",
    "print(future_steps[:5])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
